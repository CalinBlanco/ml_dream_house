{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as snb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalamos los datos del dataset\n",
    "data_train = pd.read_csv('house_train_raw.csv', encoding='utf-8', sep=',')\n",
    "data_test = pd.read_csv('houses_test_raw.csv', encoding='utf-8', sep=',')\n",
    "dataf = pd.concat((data_train, data_test)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos inforación del dataset\n",
    "dataf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el número de registros y columnas del dataset\n",
    "dataf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos en pantalla los 5 primeros registros\n",
    "dataf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que devuleva un dataframe con el conteo y porcentaje de nulos\n",
    "def porcentaje_nulos(col=''):\n",
    "  null_counts = pd.DataFrame()\n",
    "  null_counts['conteo'] = dataf.isnull().sum()\n",
    "  null_counts = null_counts[null_counts['conteo']>0].reset_index()\n",
    "  null_counts['porcentaje'] = (null_counts['conteo']/dataf.shape[0] * 100).round(2)\n",
    "  if col in ['','all']:\n",
    "    return null_counts\n",
    "  return null_counts['porcentaje'].loc[null_counts['index'] == col][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos la columna ['MSZoning'] con la moda, ya que el porcentaje de nulos es menos del 1%\n",
    "dataf['MSZoning']= dataf['MSZoning'].fillna(dataf['MSZoning'].mode()[0])\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos la columna 'LotFrontage' con su promedio porque es numérico y el porcentaje de nulos es menos del 17%\n",
    "dataf['LotFrontage']= dataf['LotFrontage'].fillna(dataf['LotFrontage'].mean())\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna, ya que contiene un 93.22 % de nulos y a mi parecer no va a influir mucho en el resultado final\n",
    "# porcentaje_nulos('Alley') -----> 93.22 %\n",
    "dataf.drop(['Alley'], axis=1, inplace=True)\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos las columnas ['Utilities','Exterior1st','Exterior2nd','MasVnrType','MasVnrArea'] con la moda, ya que el porcentaje de nulos es menos del 1% cada uno.\n",
    "columnas = ['Utilities','Exterior1st','Exterior2nd','MasVnrType','MasVnrArea']\n",
    "for columna in columnas:\n",
    "  dataf[columna] = dataf[columna].fillna(dataf[columna].mode()[0])\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos las columnas con la moda ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF'] porque son de tipo objeto\n",
    "# además los nulos de cada columna no pasan del 3% cada uno.\n",
    "\n",
    "columnas = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']\n",
    "for col in columnas:\n",
    "  # print(f\"porcentaje_nulos('{col}') -----> {porcentaje_nulos(col)} %\")\n",
    "  dataf[col] = dataf[col].fillna(dataf[col].mode()[0])\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Rellenamos las columnas con la moda ['Electrical','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType'] porque es de tipo objeto, además no pasan del 1% cada uno.\n",
    "columnas = ['Electrical','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType']\n",
    "for columna in columnas:\n",
    "  dataf[columna] = dataf[columna].fillna(dataf[columna].mode()[0])\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminanos los registros que no contienen valor en el año de construcción en la columna ['GarageYrBlt']\n",
    "dataf.dropna(subset=['GarageYrBlt'], inplace=True)\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos los valor nulos por el valor NA que significa (No Fireplace)\n",
    "dataf['FireplaceQu'] = dataf['FireplaceQu'].fillna('NA')\n",
    "# dataf['FireplaceQu'].value_counts()\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas ['PoolQC','Fence','MiscFeature'], porque sobrepasan el 80% de nulos.\n",
    "columnas = ['PoolQC','Fence','MiscFeature']\n",
    "dataf.drop(columns=columnas, axis=1, inplace=True)\n",
    "porcentaje_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si hay duplicados\n",
    "dataf.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a verificar el número de registros y columnas\n",
    "dataf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recodificamos la columna ['MSSubClass'] para tomarlo en cuenta como categórico y no numérico\n",
    "dataf['MSSubClass'] = dataf['MSSubClass'].apply(lambda x : \"MSSC\"+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperamos las columnas de tipo Objeto y numérico\n",
    "col_objets = []\n",
    "for i in dataf.columns:\n",
    "    if dataf[i].dtype == object:\n",
    "        col_objets.append(i)\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "col_numerics = []\n",
    "for i in dataf.columns:\n",
    "    if dataf[i].dtype in numeric_dtypes:\n",
    "        col_numerics.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"El número de columas de tipo objeto son: {len(col_objets)}\")\n",
    "print(f\"El número de columas de tipo numérico son: {len(col_numerics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos sólo las columnas que son numéricas\n",
    "df_final_numerics = dataf[col_numerics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que me permite aplicar Onehotencoder\n",
    "def category_onehotencoder(cols, df_encoder):\n",
    "    onehot_encoder = OneHotEncoder(sparse = False)\n",
    "    df_ = df_final_numerics.copy()\n",
    "    for col in cols:\n",
    "        col_encoder = df_encoder[col].values.reshape(-1,1)\n",
    "        onehot_encoder.fit(col_encoder)\n",
    "        cols_encoded = onehot_encoder.transform(col_encoder)\n",
    "\n",
    "        for i, e in enumerate(onehot_encoder.categories_[0]):\n",
    "            df_[e+\"_encode\"] = cols_encoded[:,i]\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos a la variable 'df_final_encode' el resultado del Onehotencoder\n",
    "df_final_encode = category_onehotencoder(col_objets,dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando con modelo Árbol de Decisión (onehotencoder y numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos los registros que contengan valor en la columna ['SalePrice] para diferenciar el dataframe train con el test\n",
    "conteo_train_ad = len(df_final_encode[(~df_final_encode.SalePrice.isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando X_df_train_ad e X_df_test_ad\n",
    "X_df_train_ad = df_final_encode.iloc[:conteo_train_ad,:]\n",
    "y_df_train_ad = X_df_train_ad.SalePrice.values\n",
    "\n",
    "X_df_test_ad = df_final_encode.iloc[conteo_train_ad:,:]\n",
    "X_df_test_ad_id = X_df_test_ad.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna ['Id','SalePrice] de ambos dataframes\n",
    "X_df_train_ad.drop(['Id','SalePrice'], axis=1, inplace=True)\n",
    "X_df_test_ad.drop(['Id','SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto árbol y hacemos el split del dataframe\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "X_train_ad, X_test_ad, y_train_ad, y_test_ad = train_test_split(X_df_train_ad, y_df_train_ad, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "tree.fit(X_train_ad, y_train_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred_ad = tree.predict(X_train_ad)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred_ad = tree.predict(X_test_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('La raíz cuadrada del error logarítmico medio (RMSLE) sobre el dataset houses_train_raw es:', mean_squared_log_error(y_test_pred_ad, y_test_ad, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_pred_ad, y_test_ad, c=\"r\", alpha=0.5)\n",
    "plt.xlabel(\"y_test_pred_ad\")\n",
    "plt.ylabel(\"y_test_ad\")\n",
    "# plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecimos la columna SalePrice del dataset \"houses_test_raw.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_raw = tree.predict(X_df_test_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_ad = pd.DataFrame(y_test_pred_raw, columns=['SalePrice'])\n",
    "X_df_test_ad_id.reset_index(drop=True, inplace=True)\n",
    "pred_test_ad.reset_index(drop=True, inplace=True)\n",
    "pred_concat_ad = pd.concat( [X_df_test_ad_id, pred_test_ad], axis=1) \n",
    "\n",
    "pred_concat_ad.to_csv(\"pred_test_ad.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando el modelo \"Extreme Gradient Boosting\"\n",
    "https://medium.com/@jboscomendoza/tutorial-xgboost-en-python-53e48fc58f73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos los registros que contengan valor en la columna ['SalePrice] para diferenciar el dataframe train con el test\n",
    "conteo_train_xgb = len(df_final_encode[(~df_final_encode.SalePrice.isnull())])\n",
    "\n",
    "# Separando X_df_train_xgb e X_df_test_xgb\n",
    "X_df_train_xgb = df_final_encode.iloc[:conteo_train_xgb,:]\n",
    "y_df_train_xgb = X_df_train_xgb.SalePrice.values\n",
    "\n",
    "X_df_test_xgb = df_final_encode.iloc[conteo_train_xgb:,:]\n",
    "X_df_test_xgb_id = X_df_test_xgb.iloc[:,:1]\n",
    "\n",
    "# Eliminamos la columna ['Id','SalePrice] de ambos dataframes\n",
    "X_df_train_xgb.drop(['Id','SalePrice'], axis=1, inplace=True)\n",
    "X_df_test_xgb.drop(['Id','SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto XGBRegressor y hacemos el split\n",
    "modelo_xgb = xgb.XGBRegressor()\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_df_train_ad, y_df_train_ad, test_size=0.33, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo XGBRegressor\n",
    "modelo_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred_xgb = modelo_xgb.predict(X_train_xgb)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred_xgb = modelo_xgb.predict(X_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el la raíz cuadrada del error logarítmico medio\n",
    "print('La raíz cuadrada del error logarítmico medio (RMSLE) sobre el dataset houses_train_raw es:', mean_squared_log_error(y_test_pred_xgb,y_test_xgb, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_pred_xgb, y_test_xgb, c=\"b\", alpha=0.5)\n",
    "plt.xlabel(\"y_test_pred_xgb\")\n",
    "plt.ylabel(\"y_test_xgb\")\n",
    "# plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos el target 'SalePrice' para el dataset houses_test_raw\n",
    "xgb_preds_test=modelo_xgb.predict(X_df_test_xgb)\n",
    "pd.DataFrame(xgb_preds_test, columns=['SalePrice']).to_csv(\"pred_test.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_xgb = pd.DataFrame(xgb_preds_test, columns=['SalePrice'])\n",
    "\n",
    "X_df_test_xgb_id.reset_index(drop=True, inplace=True)\n",
    "pred_test_xgb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pred_concat_xgb = pd.concat( [X_df_test_xgb_id, pred_test_xgb], axis=1) \n",
    "\n",
    "pred_concat_xgb.to_csv(\"pred_test.csv\", index=False, sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
